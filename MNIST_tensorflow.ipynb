{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-9dba3d52d729>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#Data input\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "55000\n"
     ]
    }
   ],
   "source": [
    "#train image\n",
    "print(mnist.train.images)\n",
    "print(len(mnist.train.images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "55000\n"
     ]
    }
   ],
   "source": [
    "#train label\n",
    "print(mnist.train.labels)\n",
    "print(len(mnist.train.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#첫번째 레이블 데이터 확인\n",
    "print(len(mnist.train.labels[0]))\n",
    "print(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "step= 0  loss= 464.98083  acc= 0.0993\n",
      "step= 100  loss= 55.826004  acc= 0.8725\n",
      "step= 200  loss= 34.281128  acc= 0.9116\n",
      "step= 300  loss= 11.065873  acc= 0.9337\n",
      "step= 400  loss= 12.939747  acc= 0.9455\n",
      "step= 500  loss= 8.577711  acc= 0.9511\n",
      "step= 600  loss= 25.6171  acc= 0.9557\n",
      "step= 700  loss= 3.3858705  acc= 0.9613\n",
      "step= 800  loss= 8.006658  acc= 0.9634\n",
      "step= 900  loss= 6.199937  acc= 0.9675\n",
      "step= 1000  loss= 17.926603  acc= 0.9705\n",
      "step= 1100  loss= 4.4213977  acc= 0.9718\n",
      "step= 1200  loss= 17.938478  acc= 0.9733\n",
      "step= 1300  loss= 10.216834  acc= 0.9736\n",
      "step= 1400  loss= 4.534616  acc= 0.9756\n",
      "step= 1500  loss= 3.47437  acc= 0.9767\n",
      "step= 1600  loss= 16.357033  acc= 0.9781\n",
      "step= 1700  loss= 0.88176775  acc= 0.9773\n",
      "step= 1800  loss= 6.7876563  acc= 0.9792\n",
      "step= 1900  loss= 1.7074162  acc= 0.9802\n",
      "step= 2000  loss= 6.0238767  acc= 0.9784\n",
      "step= 2100  loss= 13.959443  acc= 0.9789\n",
      "step= 2200  loss= 2.177008  acc= 0.981\n",
      "step= 2300  loss= 5.3928976  acc= 0.9817\n",
      "step= 2400  loss= 0.701755  acc= 0.9808\n",
      "step= 2500  loss= 3.29117  acc= 0.9824\n",
      "step= 2600  loss= 3.1201882  acc= 0.9826\n",
      "step= 2700  loss= 3.968974  acc= 0.9837\n",
      "step= 2800  loss= 14.464757  acc= 0.984\n",
      "step= 2900  loss= 4.738261  acc= 0.985\n",
      "step= 3000  loss= 8.529068  acc= 0.9841\n",
      "step= 3100  loss= 0.53186893  acc= 0.9851\n",
      "step= 3200  loss= 3.8973284  acc= 0.9851\n",
      "step= 3300  loss= 5.4893913  acc= 0.9867\n",
      "step= 3400  loss= 1.6034658  acc= 0.9857\n",
      "step= 3500  loss= 4.826953  acc= 0.9849\n",
      "step= 3600  loss= 5.129351  acc= 0.983\n",
      "step= 3700  loss= 1.6036953  acc= 0.9859\n",
      "step= 3800  loss= 1.60925  acc= 0.9877\n",
      "step= 3900  loss= 1.5104549  acc= 0.9853\n",
      "step= 4000  loss= 0.9073398  acc= 0.9881\n",
      "step= 4100  loss= 1.548951  acc= 0.987\n",
      "step= 4200  loss= 0.8595959  acc= 0.9848\n",
      "step= 4300  loss= 0.5092486  acc= 0.9865\n",
      "step= 4400  loss= 4.9344044  acc= 0.988\n",
      "step= 4500  loss= 4.8168344  acc= 0.9884\n",
      "step= 4600  loss= 0.19781704  acc= 0.9876\n",
      "step= 4700  loss= 0.2624939  acc= 0.9883\n",
      "step= 4800  loss= 7.511554  acc= 0.987\n",
      "step= 4900  loss= 2.806356  acc= 0.9879\n",
      "step= 5000  loss= 3.013903  acc= 0.9893\n",
      "step= 5100  loss= 0.42962515  acc= 0.9889\n",
      "step= 5200  loss= 3.8837957  acc= 0.9879\n",
      "step= 5300  loss= 7.1566024  acc= 0.9886\n",
      "step= 5400  loss= 0.8865607  acc= 0.9893\n",
      "step= 5500  loss= 6.663302  acc= 0.9889\n",
      "step= 5600  loss= 3.0459468  acc= 0.9883\n",
      "step= 5700  loss= 3.9743166  acc= 0.988\n",
      "step= 5800  loss= 2.1812735  acc= 0.9884\n",
      "step= 5900  loss= 0.6042232  acc= 0.9896\n",
      "step= 6000  loss= 1.4534308  acc= 0.9899\n",
      "step= 6100  loss= 0.6720679  acc= 0.9893\n",
      "step= 6200  loss= 4.7286325  acc= 0.9896\n",
      "step= 6300  loss= 1.2201324  acc= 0.9887\n",
      "step= 6400  loss= 1.5870237  acc= 0.9906\n",
      "step= 6500  loss= 0.5725363  acc= 0.9905\n",
      "step= 6600  loss= 1.4631872  acc= 0.9901\n",
      "step= 6700  loss= 1.6197112  acc= 0.9884\n",
      "step= 6800  loss= 0.51549536  acc= 0.99\n",
      "step= 6900  loss= 0.33631623  acc= 0.9896\n",
      "step= 7000  loss= 0.88566834  acc= 0.9895\n",
      "step= 7100  loss= 0.1883106  acc= 0.9915\n",
      "step= 7200  loss= 3.194804  acc= 0.9909\n",
      "step= 7300  loss= 0.65561473  acc= 0.9912\n",
      "step= 7400  loss= 2.1129103  acc= 0.9886\n",
      "step= 7500  loss= 0.68265986  acc= 0.9894\n",
      "step= 7600  loss= 2.1231701  acc= 0.9902\n",
      "step= 7700  loss= 5.591541  acc= 0.9899\n",
      "step= 7800  loss= 2.4936469  acc= 0.9915\n",
      "step= 7900  loss= 0.19620627  acc= 0.9915\n",
      "step= 8000  loss= 0.6225465  acc= 0.9911\n",
      "step= 8100  loss= 0.47353166  acc= 0.99\n",
      "step= 8200  loss= 0.6430268  acc= 0.9893\n",
      "step= 8300  loss= 0.32879815  acc= 0.9892\n",
      "step= 8400  loss= 1.1807888  acc= 0.9909\n",
      "step= 8500  loss= 0.21539533  acc= 0.9897\n",
      "step= 8600  loss= 0.6989038  acc= 0.9909\n",
      "step= 8700  loss= 0.2881612  acc= 0.9901\n",
      "step= 8800  loss= 2.385072  acc= 0.991\n",
      "step= 8900  loss= 3.3138475  acc= 0.9904\n",
      "step= 9000  loss= 2.7228503  acc= 0.991\n",
      "step= 9100  loss= 0.12842539  acc= 0.9921\n",
      "step= 9200  loss= 2.6031513  acc= 0.9914\n",
      "step= 9300  loss= 0.27550822  acc= 0.9917\n",
      "step= 9400  loss= 0.09095799  acc= 0.9913\n",
      "step= 9500  loss= 1.5481716  acc= 0.9916\n",
      "step= 9600  loss= 0.73196864  acc= 0.9915\n",
      "step= 9700  loss= 0.63049424  acc= 0.991\n",
      "step= 9800  loss= 0.06119609  acc= 0.991\n",
      "step= 9900  loss= 0.15586185  acc= 0.9903\n",
      "정답률= 0.991\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#MNIST 데이터 읽어들이기\n",
    "mnist=input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "\n",
    "pixels=28*28 \n",
    "nums=10 # 0-9사이의 카테고리\n",
    "\n",
    "#placeholder 정의하기\n",
    "x=tf.placeholder(tf.float32,shape=(None, pixels), name=\"x\") #image data\n",
    "y_=tf.placeholder(tf.float32,shape=(None, nums), name=\"y_\") #result label\n",
    "\n",
    "#가중치와 바이어스를 초기화하는 함수\n",
    "def weight_variable(name, shape):\n",
    "    W_init=tf.truncated_normal(shape,stddev=0.1)\n",
    "    W=tf.Variable(W_init, name=\"W_\"+name)\n",
    "    return W\n",
    "\n",
    "def bias_variable(name, size):\n",
    "    b_init=tf.constant(0,1,shape=[size])\n",
    "    b=tf.Variable(b_init, name=\"b_\"+name)\n",
    "    return b\n",
    "\n",
    "#합성곱 계층을 만드는 함수\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "#최대 풀링층\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "#합성곱층1\n",
    "with tf.name_scope('conv1') as scope:\n",
    "    W_conv1=weight_variable('conv1',[5,5,1,32]) # 5*5 filter, 입력 채녈 1 (흑백), 출력 채녈 32\n",
    "    b_conv1=bias_variable('conv1',32)\n",
    "    x_image=tf.reshape(x,[-1,28,28,1])\n",
    "    h_conv1=tf.nn.relu(conv2d(x_image, W_conv1)+b_conv1)\n",
    "    \n",
    "#풀링층1\n",
    "with tf.name_scope('pool1') as scope:\n",
    "    h_pool1=max_pool(h_conv1)\n",
    "    \n",
    "#합성곱층2\n",
    "with tf.name_scope('conv2') as scope:\n",
    "    W_conv2=weight_variable('conv2', [5,5,32,64])\n",
    "    b_conv2=bias_variable('conv2', 64)\n",
    "    h_conv2=tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    \n",
    "#풀링층2\n",
    "with tf.name_scope('pool2') as scope:\n",
    "    h_pool2=max_pool(h_conv2)\n",
    "    \n",
    "#fully connected\n",
    "with tf.name_scope('fully_connected') as scope:\n",
    "    n=7*7*64 #2*2 pooling 2번 = 28/2/2 -> 7*7\n",
    "    W_fc=weight_variable('fc', [n,1024])\n",
    "    b_fc=bias_variable('fc', 1024)\n",
    "    h_pool2_flat=tf.reshape(h_pool2, [-1,n])\n",
    "    h_fc=tf.nn.relu(tf.matmul(h_pool2_flat, W_fc)+b_fc)\n",
    "    \n",
    "#dropout\n",
    "with tf.name_scope('dropout') as scope:\n",
    "    keep_prob=tf.placeholder(tf.float32)\n",
    "    h_fc_drop=tf.nn.dropout(h_fc, keep_prob)\n",
    "    \n",
    "#출력층\n",
    "with tf.name_scope('readout') as scope:\n",
    "    W_fc2=weight_variable('fc2', [1024,10])\n",
    "    b_fc2=bias_variable('fc2',10)\n",
    "    y_conv=tf.nn.softmax(tf.matmul(h_fc_drop, W_fc2)+b_fc2)\n",
    "    \n",
    "#모델 학습시키기\n",
    "with tf.name_scope('loss') as scope:\n",
    "    cross_entoropy=-tf.reduce_sum(y_*tf.log(y_conv))\n",
    "with tf.name_scope('training') as scope:\n",
    "    optimizer=tf.train.AdamOptimizer(1e-4) # 확률적 SGD\n",
    "    train_step=optimizer.minimize(cross_entoropy)\n",
    "    \n",
    "#모델 평가하기\n",
    "with tf.name_scope('predict') as scope:\n",
    "    predict_step=tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy_step=tf.reduce_mean(tf.cast(predict_step, tf.float32))\n",
    "    \n",
    "#feed_dict 설정하기\n",
    "def set_feed(images,labels, prob):\n",
    "    return {x:images, y_:labels, keep_prob: prob}\n",
    "\n",
    "#세션 시작하기\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Tensorboard \n",
    "    tw=tf.summary.FileWriter(\"log_dir\",graph=sess.graph)\n",
    "    #test 전용 피드\n",
    "    test_fd=set_feed(mnist.test.images, mnist.test.labels,1)\n",
    "    #학습시작\n",
    "    for step in range(10000):\n",
    "        batch=mnist.train.next_batch(50) #50개 이미지 10000번 학습\n",
    "        fd=set_feed(batch[0], batch[1], 0.5)\n",
    "        _, loss=sess.run([train_step, cross_entoropy], feed_dict=fd)\n",
    "        if step % 100 ==0:\n",
    "            acc=sess.run(accuracy_step,feed_dict=test_fd)\n",
    "            print(\"step=\", step, \" loss=\", loss, \" acc=\", acc)\n",
    "            \n",
    "    #최종결과출력\n",
    "    acc=sess.run(accuracy_step, feed_dict=test_fd)\n",
    "    print(\"정답률=\",acc)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
